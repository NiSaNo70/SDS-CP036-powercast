{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259b1498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training and Asset Generation ---\n",
      "\n",
      "--- Scaler has been fitted and saved to scaler.pkl ---\n",
      "\n",
      "================================================================================\n",
      "--- Processing Zone 1 (Target: Zone_1_Power_Consumption) ---\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50 | Train Loss: 0.005940 | Validation Loss: 0.000806\n",
      "Epoch 2/50 | Train Loss: 0.000575 | Validation Loss: 0.001401\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/50 | Train Loss: 0.000438 | Validation Loss: 0.000272\n",
      "Epoch 4/50 | Train Loss: 0.000359 | Validation Loss: 0.000236\n",
      "Epoch 5/50 | Train Loss: 0.000297 | Validation Loss: 0.000244\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/50 | Train Loss: 0.000278 | Validation Loss: 0.000223\n",
      "Epoch 7/50 | Train Loss: 0.000236 | Validation Loss: 0.000149\n",
      "Epoch 8/50 | Train Loss: 0.000227 | Validation Loss: 0.000149\n",
      "Epoch 9/50 | Train Loss: 0.000203 | Validation Loss: 0.000146\n",
      "Epoch 10/50 | Train Loss: 0.000204 | Validation Loss: 0.000126\n",
      "Epoch 11/50 | Train Loss: 0.000189 | Validation Loss: 0.000162\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/50 | Train Loss: 0.000184 | Validation Loss: 0.000168\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/50 | Train Loss: 0.000187 | Validation Loss: 0.000100\n",
      "Epoch 14/50 | Train Loss: 0.000181 | Validation Loss: 0.000126\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/50 | Train Loss: 0.000174 | Validation Loss: 0.000099\n",
      "Epoch 16/50 | Train Loss: 0.000171 | Validation Loss: 0.000214\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/50 | Train Loss: 0.000165 | Validation Loss: 0.000164\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/50 | Train Loss: 0.000174 | Validation Loss: 0.000239\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/50 | Train Loss: 0.000158 | Validation Loss: 0.000332\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 20/50 | Train Loss: 0.000140 | Validation Loss: 0.000093\n",
      "Epoch 21/50 | Train Loss: 0.000143 | Validation Loss: 0.000106\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/50 | Train Loss: 0.000141 | Validation Loss: 0.000086\n",
      "Epoch 23/50 | Train Loss: 0.000142 | Validation Loss: 0.000098\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 24/50 | Train Loss: 0.000141 | Validation Loss: 0.000121\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 25/50 | Train Loss: 0.000138 | Validation Loss: 0.000146\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 26/50 | Train Loss: 0.000142 | Validation Loss: 0.000121\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 27/50 | Train Loss: 0.000129 | Validation Loss: 0.000116\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered!\n",
      "\n",
      "--- Training Complete for Zone 1 ---\n",
      "\n",
      "--- Final Test Set Performance for Zone 1 ---\n",
      "Mean Absolute Error (MAE): 229.91 kW\n",
      "Root Mean Squared Error (RMSE): 328.76 kW\n",
      "R-squared (R²): 0.9971\n",
      "--- Model for Zone 1 saved to model_Zone_1.pth ---\n",
      "\n",
      "================================================================================\n",
      "--- Processing Zone 2 (Target: Zone_2__Power_Consumption) ---\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50 | Train Loss: 0.003578 | Validation Loss: 0.000467\n",
      "Epoch 2/50 | Train Loss: 0.000457 | Validation Loss: 0.001016\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/50 | Train Loss: 0.000370 | Validation Loss: 0.000446\n",
      "Epoch 4/50 | Train Loss: 0.000299 | Validation Loss: 0.000303\n",
      "Epoch 5/50 | Train Loss: 0.000255 | Validation Loss: 0.000190\n",
      "Epoch 6/50 | Train Loss: 0.000222 | Validation Loss: 0.000184\n",
      "Epoch 7/50 | Train Loss: 0.000201 | Validation Loss: 0.000261\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 8/50 | Train Loss: 0.000179 | Validation Loss: 0.000268\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 9/50 | Train Loss: 0.000178 | Validation Loss: 0.000137\n",
      "Epoch 10/50 | Train Loss: 0.000163 | Validation Loss: 0.000120\n",
      "Epoch 11/50 | Train Loss: 0.000162 | Validation Loss: 0.000818\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/50 | Train Loss: 0.000160 | Validation Loss: 0.000125\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/50 | Train Loss: 0.000150 | Validation Loss: 0.000107\n",
      "Epoch 14/50 | Train Loss: 0.000156 | Validation Loss: 0.000162\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/50 | Train Loss: 0.000144 | Validation Loss: 0.000100\n",
      "Epoch 16/50 | Train Loss: 0.000140 | Validation Loss: 0.000103\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/50 | Train Loss: 0.000141 | Validation Loss: 0.000105\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/50 | Train Loss: 0.000134 | Validation Loss: 0.000138\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/50 | Train Loss: 0.000133 | Validation Loss: 0.000148\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 20/50 | Train Loss: 0.000116 | Validation Loss: 0.000094\n",
      "Epoch 21/50 | Train Loss: 0.000117 | Validation Loss: 0.000141\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/50 | Train Loss: 0.000118 | Validation Loss: 0.000164\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 23/50 | Train Loss: 0.000116 | Validation Loss: 0.000083\n",
      "Epoch 24/50 | Train Loss: 0.000118 | Validation Loss: 0.000132\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 25/50 | Train Loss: 0.000114 | Validation Loss: 0.000087\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 26/50 | Train Loss: 0.000112 | Validation Loss: 0.000086\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 27/50 | Train Loss: 0.000115 | Validation Loss: 0.000082\n",
      "Epoch 28/50 | Train Loss: 0.000110 | Validation Loss: 0.000084\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/50 | Train Loss: 0.000112 | Validation Loss: 0.000141\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 30/50 | Train Loss: 0.000112 | Validation Loss: 0.000091\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 31/50 | Train Loss: 0.000110 | Validation Loss: 0.000094\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 32/50 | Train Loss: 0.000102 | Validation Loss: 0.000109\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered!\n",
      "\n",
      "--- Training Complete for Zone 2 ---\n",
      "\n",
      "--- Final Test Set Performance for Zone 2 ---\n",
      "Mean Absolute Error (MAE): 317.10 kW\n",
      "Root Mean Squared Error (RMSE): 437.89 kW\n",
      "R-squared (R²): 0.9939\n",
      "--- Model for Zone 2 saved to model_Zone_2.pth ---\n",
      "\n",
      "================================================================================\n",
      "--- Processing Zone 3 (Target: Zone_3__Power_Consumption) ---\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50 | Train Loss: 0.002463 | Validation Loss: 0.000261\n",
      "Epoch 2/50 | Train Loss: 0.000332 | Validation Loss: 0.000176\n",
      "Epoch 3/50 | Train Loss: 0.000247 | Validation Loss: 0.000277\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/50 | Train Loss: 0.000213 | Validation Loss: 0.000249\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5/50 | Train Loss: 0.000176 | Validation Loss: 0.000234\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 6/50 | Train Loss: 0.000155 | Validation Loss: 0.000131\n",
      "Epoch 7/50 | Train Loss: 0.000141 | Validation Loss: 0.000109\n",
      "Epoch 8/50 | Train Loss: 0.000132 | Validation Loss: 0.000114\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/50 | Train Loss: 0.000125 | Validation Loss: 0.000109\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 10/50 | Train Loss: 0.000119 | Validation Loss: 0.000252\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 11/50 | Train Loss: 0.000107 | Validation Loss: 0.000089\n",
      "Epoch 12/50 | Train Loss: 0.000100 | Validation Loss: 0.000228\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/50 | Train Loss: 0.000104 | Validation Loss: 0.000134\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 14/50 | Train Loss: 0.000094 | Validation Loss: 0.000091\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 15/50 | Train Loss: 0.000087 | Validation Loss: 0.000123\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 16/50 | Train Loss: 0.000072 | Validation Loss: 0.000207\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered!\n",
      "\n",
      "--- Training Complete for Zone 3 ---\n",
      "\n",
      "--- Final Test Set Performance for Zone 3 ---\n",
      "Mean Absolute Error (MAE): 725.08 kW\n",
      "Root Mean Squared Error (RMSE): 826.91 kW\n",
      "R-squared (R²): 0.9397\n",
      "--- Model for Zone 3 saved to model_Zone_3.pth ---\n",
      "\n",
      "--- All models trained and assets saved. Deployment preparation is complete. ---\n"
     ]
    }
   ],
   "source": [
    "# Tetuan City Power Consumption: Train models and save assets for deployment\n",
    "# This script is designed to be run once. Its purpose is to train the final\n",
    "# machine learning models and save all the necessary components (assets)\n",
    "# that a deployment application (like a Streamlit app or an API) will need\n",
    "# to make live predictions without having to retrain.\n",
    "\n",
    "# --- Core Library Imports ---\n",
    "import pandas as pd  # For data manipulation and reading CSV files.\n",
    "import numpy as np  # For numerical operations, especially with arrays.\n",
    "import torch  # The main PyTorch library for building neural networks.\n",
    "import torch.nn as nn  # Contains the building blocks for neural networks (layers, loss functions, etc.).\n",
    "from torch.utils.data import TensorDataset, DataLoader  # For creating efficient data pipelines.\n",
    "from sklearn.preprocessing import MinMaxScaler  # For scaling numerical features to a 0-1 range.\n",
    "from sklearn import metrics  # For evaluating model performance (MAE, RMSE, R²).\n",
    "import pickle  # Used for serializing and saving Python objects, like our scaler.\n",
    "\n",
    "print(\"--- Starting Training and Asset Generation ---\")\n",
    "\n",
    "# --- 1. Data Loading and Preparation ---\n",
    "# This section handles loading the raw data, cleaning it, and performing\n",
    "# the necessary feature engineering steps.\n",
    "\n",
    "try:\n",
    "    # Load the dataset from the specified CSV file.\n",
    "    df = pd.read_csv('Tetuan City power consumption.csv')\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the data file is not found.\n",
    "    print(\"Error: 'Tetuan City power consumption.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Clean column names by removing leading/trailing spaces and replacing single spaces with underscores.\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "# Convert the 'DateTime' column from a string to a proper datetime object.\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "# Set the 'DateTime' column as the index of the DataFrame, which is essential for time-series analysis.\n",
    "df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Define the list of raw features we will use for our model.\n",
    "features_to_use = [\n",
    "    'Temperature', 'Humidity', 'Wind_Speed', 'general_diffuse_flows',\n",
    "    'Zone_1_Power_Consumption', 'Zone_2__Power_Consumption', 'Zone_3__Power_Consumption'\n",
    "]\n",
    "# Create a new DataFrame 'data' containing only the columns we need.\n",
    "data = df[features_to_use].copy()\n",
    "\n",
    "# --- Feature Engineering: Cyclical Time Features ---\n",
    "# We convert cyclical time features (like hour of the day) into a format that\n",
    "# machine learning models can understand. 23:00 is close to 00:00, and sine/cosine\n",
    "# transformations represent this circular relationship.\n",
    "data['hour_sin'] = np.sin(2 * np.pi * data.index.hour / 24.0)\n",
    "data['hour_cos'] = np.cos(2 * np.pi * data.index.hour / 24.0)\n",
    "data['dayofweek_sin'] = np.sin(2 * np.pi * data.index.dayofweek / 7.0)\n",
    "data['dayofweek_cos'] = np.cos(2 * np.pi * data.index.dayofweek / 7.0)\n",
    "\n",
    "# Store the final list of feature names for later use (e.g., in SHAP plots).\n",
    "FEATURE_NAMES = data.columns.tolist()\n",
    "\n",
    "# --- Data Splitting (Chronological) ---\n",
    "# For time-series data, it's crucial to split the data chronologically to prevent\n",
    "# data leakage (i.e., using future information to train the model).\n",
    "train_size = int(len(data) * 0.7)\n",
    "val_size = int(len(data) * 0.15)\n",
    "# The remaining 15% will be the test set.\n",
    "train_df = data.iloc[:train_size]\n",
    "val_df = data.iloc[train_size:train_size + val_size]\n",
    "test_df = data.iloc[train_size + val_size:]\n",
    "\n",
    "# --- Scaling ---\n",
    "# We scale the data to a range between 0 and 1. This helps the neural network\n",
    "# train more effectively, as it prevents features with large values from dominating\n",
    "# the learning process.\n",
    "scaler = MinMaxScaler()\n",
    "# IMPORTANT: We fit the scaler ONLY on the training data to avoid data leakage from the validation and test sets.\n",
    "scaler.fit(train_df)\n",
    "# Then, we transform all three sets using the fitted scaler.\n",
    "train_scaled = scaler.transform(train_df)\n",
    "val_scaled = scaler.transform(val_df)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# --- SAVE ASSET 1: THE SCALER ---\n",
    "# The scaler is a critical asset. Any new data for prediction must be scaled\n",
    "# in the exact same way as the training data. We save it using pickle.\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"\\n--- Scaler has been fitted and saved to scaler.pkl ---\")\n",
    "\n",
    "\n",
    "# --- 2. Training Strategies: Early Stopping Class ---\n",
    "# This helper class is used to monitor the model's performance on the validation set\n",
    "# and stop the training process if it stops improving. This prevents overfitting\n",
    "# and saves computational time.\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience  # How many epochs to wait for improvement before stopping.\n",
    "        self.verbose = verbose  # If True, prints a message for each patience count.\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta  # A small value to consider an improvement.\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# ############################################################################\n",
    "# This is the main part of the script. It loops through each Zone,\n",
    "# trains a dedicated model, and saves its weights to a file.\n",
    "# ############################################################################\n",
    "\n",
    "# A dictionary to map user-friendly zone names to their column names in the DataFrame.\n",
    "TARGET_ZONES = {\n",
    "    'Zone 1': 'Zone_1_Power_Consumption',\n",
    "    'Zone 2': 'Zone_2__Power_Consumption', # Note the double underscore from initial cleaning\n",
    "    'Zone 3': 'Zone_3__Power_Consumption'\n",
    "}\n",
    "\n",
    "# Main loop to train a model for each zone.\n",
    "for i, (zone_name, target_col_name) in enumerate(TARGET_ZONES.items()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"--- Processing {zone_name} (Target: {target_col_name}) ---\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # --- Sequence Creation Function ---\n",
    "    # This function converts the flat time-series data into sequences (or windows)\n",
    "    # that are suitable for an LSTM model.\n",
    "    def create_sequences(input_data, lookback_window, target_column_index):\n",
    "        X, y = [], []\n",
    "        # Iterate through the data to create overlapping windows.\n",
    "        for i in range(len(input_data) - lookback_window):\n",
    "            # The input sequence (X) is a window of 'lookback_window' time steps.\n",
    "            sequence = input_data[i:(i + lookback_window)]\n",
    "            # The target (y) is the value of the target column immediately after the sequence.\n",
    "            target = input_data[i + lookback_window, target_column_index]\n",
    "            X.append(sequence)\n",
    "            y.append(target)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Define the lookback window: how many previous time steps the model should see to make a prediction.\n",
    "    # 144 steps * 10 minutes/step = 1440 minutes = 24 hours.\n",
    "    LOOKBACK_WINDOW = 144\n",
    "    # Get the numerical index of our target column.\n",
    "    TARGET_COL_IDX = train_df.columns.get_loc(target_col_name)\n",
    "\n",
    "    # Create the sequences for training, validation, and testing.\n",
    "    X_train, y_train = create_sequences(train_scaled, LOOKBACK_WINDOW, TARGET_COL_IDX)\n",
    "    X_val, y_val = create_sequences(val_scaled, LOOKBACK_WINDOW, TARGET_COL_IDX)\n",
    "    X_test, y_test = create_sequences(test_scaled, LOOKBACK_WINDOW, TARGET_COL_IDX)\n",
    "\n",
    "    # --- Create PyTorch DataLoaders ---\n",
    "    # DataLoaders handle batching, shuffling, and efficiently loading data for the model.\n",
    "    # First, convert numpy arrays to PyTorch tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Create TensorDatasets, which pair the input tensors with their corresponding target tensors.\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Define the batch size: how many sequences are processed at once.\n",
    "    BATCH_SIZE = 64\n",
    "    # Create the final DataLoader objects.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # --- Model Architecture Definition ---\n",
    "    # This class defines the structure of our neural network. It MUST be identical\n",
    "    # in the deployment script to load the saved weights correctly.\n",
    "    class BiLSTMForecaster(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "            super(BiLSTMForecaster, self).__init__()\n",
    "            # Bidirectional LSTM layer processes the sequence in both forward and backward directions.\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,  # This means the input tensor shape is (batch, sequence, feature).\n",
    "                                dropout=dropout_rate, # Adds dropout for regularization.\n",
    "                                bidirectional=True)\n",
    "            # A final linear layer to map the LSTM output to our single prediction value.\n",
    "            # The input size is hidden_size * 2 because the bidirectional LSTM concatenates\n",
    "            # the outputs from the forward and backward passes.\n",
    "            self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # The forward pass defines how data flows through the network.\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            # We only care about the output of the very last time step in the sequence.\n",
    "            last_time_step_out = lstm_out[:, -1, :]\n",
    "            # Pass this last output through the linear layer to get the final prediction.\n",
    "            out = self.linear(last_time_step_out)\n",
    "            return out\n",
    "\n",
    "    # --- Model Hyperparameters ---\n",
    "    INPUT_SIZE = X_train.shape[2]  # The number of features in our input data.\n",
    "    HIDDEN_SIZE = 64  # The number of neurons in each LSTM layer.\n",
    "    NUM_LAYERS = 2  # The number of stacked LSTM layers.\n",
    "    OUTPUT_SIZE = 1  # We are predicting a single value.\n",
    "    DROPOUT_RATE = 0.2  # The dropout rate for regularization.\n",
    "\n",
    "    # Instantiate the model.\n",
    "    model = BiLSTMForecaster(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE, DROPOUT_RATE)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    loss_function = nn.MSELoss()  # Mean Squared Error is a common loss function for regression.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam is an efficient and popular optimizer.\n",
    "    early_stopper = EarlyStopping(patience=5, verbose=True) # Initialize our early stopper.\n",
    "    # This scheduler will reduce the learning rate if the validation loss plateaus.\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    EPOCHS = 50  # The maximum number of epochs to train for.\n",
    "\n",
    "    # Loop through each epoch.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()  # Set the model to training mode.\n",
    "        running_loss = 0.0\n",
    "        # Loop through batches of data from the train_loader.\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients.\n",
    "            output = model(batch_X)  # Forward pass.\n",
    "            loss = loss_function(output, batch_y)  # Calculate the loss.\n",
    "            loss.backward()  # Backward pass (calculate gradients).\n",
    "            optimizer.step()  # Update the model's weights.\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode.\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculation for efficiency.\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                output = model(batch_X)\n",
    "                loss = loss_function(output, batch_y)\n",
    "                running_val_loss += loss.item()\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Step the scheduler and check for early stopping.\n",
    "        scheduler.step(avg_val_loss)\n",
    "        early_stopper(avg_val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "            \n",
    "    print(f\"\\n--- Training Complete for {zone_name} ---\")\n",
    "\n",
    "    # --- Final Evaluation on the Unseen Test Set ---\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            output = model(batch_X)\n",
    "            predictions.append(output.numpy())\n",
    "            actuals.append(batch_y.numpy())\n",
    "    \n",
    "    # Concatenate results from all batches.\n",
    "    predictions = np.concatenate(predictions).flatten()\n",
    "    actuals = np.concatenate(actuals).flatten()\n",
    "\n",
    "    # --- Inverse Transform Predictions to Original Scale ---\n",
    "    # The model predicts scaled values. We need to inverse-transform them to get\n",
    "    # the actual power consumption values in kW.\n",
    "    # We create a dummy array of the correct shape, fill our target column with\n",
    "    # the predictions, and then apply the inverse transform.\n",
    "    dummy_array = np.zeros((len(predictions), len(data.columns)))\n",
    "    dummy_array[:, TARGET_COL_IDX] = predictions\n",
    "    predictions_inversed = scaler.inverse_transform(dummy_array)[:, TARGET_COL_IDX]\n",
    "\n",
    "    # Do the same for the actual values to compare.\n",
    "    dummy_array_actuals = np.zeros((len(actuals), len(data.columns)))\n",
    "    dummy_array_actuals[:, TARGET_COL_IDX] = actuals\n",
    "    actuals_inversed = scaler.inverse_transform(dummy_array_actuals)[:, TARGET_COL_IDX]\n",
    "\n",
    "    # Calculate final performance metrics.\n",
    "    mae = metrics.mean_absolute_error(actuals_inversed, predictions_inversed)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(actuals_inversed, predictions_inversed))\n",
    "    r2 = metrics.r2_score(actuals_inversed, predictions_inversed)\n",
    "    print(f\"\\n--- Final Test Set Performance for {zone_name} ---\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f} kW\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} kW\")\n",
    "    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "    # --- SAVE ASSET 2: THE MODEL WEIGHTS ---\n",
    "    # This is the final and most important asset. We save the model's learned\n",
    "    # parameters (state_dict) to a .pth file.\n",
    "    model_filename = f'model_{zone_name.replace(\" \", \"_\")}.pth'\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    print(f\"--- Model for {zone_name} saved to {model_filename} ---\")\n",
    "\n",
    "print(\"\\n--- All models trained and assets saved. Deployment preparation is complete. ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
